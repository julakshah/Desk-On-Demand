(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,34719,(e,t,o)=>{o.useMDXComponents=function(){return{}}},81694,e=>{"use strict";var t=e.i(43476);e.i(71645);var o=e.i(34719);function n(e){let n={p:"p",...(0,o.useMDXComponents)(),...e.components};return(0,t.jsx)(n.p,{children:"Write overview here"})}function s(e={}){let{wrapper:r}={...(0,o.useMDXComponents)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(n,{...e})}):n(e)}let r=()=>(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("h2",{children:"Overview"}),(0,t.jsx)(s,{})]});function i(e){let n={h4:"h4",img:"img",p:"p",...(0,o.useMDXComponents)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Write your stuff here"}),"\n",(0,t.jsx)(n.h4,{children:"Trashbot V1:"}),"\n",(0,t.jsx)(n.p,{children:"In the intrest of giving our lovely code demons as much time to test on physical hardware as possible, we began the mechanical journey with a super simple bot:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/IMG_2746.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"As you can see, this bot has little thought put into wire management, or electronics plaement.\nAll we needed from this was a testbed with the same fundamental architecture as the final trashbot, and the chair.\nAs you can see, this bot was so small that the battery didn't even fit inside!\nInstead, it was put on top.\nThis both decreased print times, and made swaping fuses easier, but was a big compromise for industrial design.\nOn top of all of this, there was no place to mount a trashcan, making it a pretty trash trashbot.\nDespite all of this, it performed it's one purpose perfectly:"}),"\n",(0,t.jsx)(n.p,{children:"Letting the code team get used to new hardware, and prove the basic mechanical architecture we would use for the rest of the project."}),"\n",(0,t.jsx)(n.p,{children:"This also gave us a larger window of time to work on the chair and trashbot V2 without the code or electrical team breathing down our necks!"}),"\n",(0,t.jsx)(n.h4,{children:"Trashbot V2:"}),"\n",(0,t.jsx)(n.p,{children:"Trashbot V2 was a complete do-over compared ot trashbot V1.\nWe had tons of feadback from the electrical and code teams to implement.\nThe bigest change was an increase in footprint.\nThis let us finaly put the batery and power distrobution board inside the robot, as well as fit some new H-bridge motor controllers.\nIt also allowed us to mount the Intel Realsense depth camera in the frame.\nThis ensured it was better protected and removing unsightly exposed cables.\nLast but certainly not least, the increased footprint allowed Trashbot V2 to actually carry a trashcan!"}),"\n",(0,t.jsx)(n.p,{children:"While the increase in size of Trashbot V2 seems exclusively like an upgrade, it makes one thing much harder:"}),"\n",(0,t.jsx)(n.p,{children:"Manufacturing:"}),"\n",(0,t.jsx)(n.p,{children:"We still wanted to use a 3D printed frame for Trashbot V2, but because of the increase in footprint, the frame no longer fit on a prusa printer.\nTo remedy this, we split the frame into 4 pieces.\nThis creates a new problem though: How do you re-connect the pieces?"}),"\n",(0,t.jsx)(n.p,{children:"For this, we designed a plate that could be laser cut which we could then screw each piece into.\nThen, in combination with a laser cut lid, we would have a strong chassis.\nAs nice as this concept seems, the clock had other ideas.\nIn the intrest of moving onto fabricating the chair, we were forced to use a different method to join the chassis pieces.\nInspired by RC glider wing designs, we made use of the very large flat surface area of the bot frame to create a strong tape joint.\nNow, I know how that sounds, but you'd be suprised with how strong this tape joint actualy is!"}),"\n",(0,t.jsx)(n.h4,{children:"Chair Fabrication:"}),"\n",(0,t.jsx)(n.p,{children:"The fabrication process for the chair began with making the frame:"}),"\n",(0,t.jsx)(n.p,{children:"First we made a cut list:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/78542965809__6B6E25D3-66FC-4EFB-A777-5FE0BB3E6129.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"We don't need anything fancy with this, as long as all the critical dimensions are clearly called out.\nWith this, we were able to use a horizontal band saw to cut all the pieces of tubing we would need for the chair.\nThis whole process took around 6 hours, and was completed over the course of 2 days."}),"\n",(0,t.jsx)(n.p,{children:"Once we had all the tubes cut to length, the next step was prepping them for welding:"}),"\n",(0,t.jsx)(n.p,{children:"TIG welding is very sensitive to contamination, so our first step of weld prepping involves removing anythign that could cause contamination.\nTo start off, we cleaned all of the coolant from the band saw off the tubes.\nThis not only entails cleaning the outside of the tubes, but the inside too.\nSince welding should melt all the way through the base metal for propper penetration, any contamination on the back side of the weld will get drawn into the weld pool.\nLuckily for us, the coolant we use for the band saw is water based, and as such, washes off very easily.\nAfter rinsing the coolant off, we used the belt sander to add a bevel to the ends off all the tubes.\nThis will promote better weld penetration, thus giving stronger welds.\nAlso, since we need to grind down some of the welds for the sake of industrial design, or fitup, the bevel prevents surface level welds that will completely dissapear when ground flush.\nThen we rinse the tubes again, this time ensuring that any aluminum dust or abrasive grit in or around the tube gets washed off.\nThe last cleaning that we need to do in a sink is degreasing.\nThis removes any oils our hands may have deposited on the surface."}),"\n",(0,t.jsx)(n.p,{children:"Now our aluminum is ready to weld right?"}),"\n",(0,t.jsx)(n.p,{children:"Nope! we still have to do a final wipe down with acetone, followed by the most cruicial step: pre-heating.\nSince aluminum conducts heat so much faster than steel, welding with cold base material is very dificult.\nThe pieces become very hard to join, and the weld puddle lags behind your torch agressively.\nThis will result in a very frustrating experience that culminates in lumpy looking, shallow welds.\nTo avoid this, all we have to do is pre-heat!"}),"\n",(0,t.jsx)(n.p,{children:"How do we know what temperature to pre-heat too?"}),"\n",(0,t.jsx)(n.p,{children:"All you need is a Sharpie.\nThicker aluminum like what we are welding generaly requires a preheat between 300 and 400 degrees.\nSharpie ink begins to burn off at around 350 degrees F, which makes it perfect for this!\nTo preheat, all we do is apply a Sharpie line along both sides of the joint we are about to weld.\nThen we heat up the area using a blow torch until the Sharpie begins to dissapear."}),"\n",(0,t.jsx)(n.p,{children:"Now we are finaly ready to lay down some beads!"}),"\n",(0,t.jsx)(n.p,{children:"To start out with, we fixtured the tubes for the side of the chair base to the table.\nWe then did the whole pre-heat routine, followed by laying down the first welds.\nOnce the first set of welds were done on each joint, we unclamped the piece from the weld table, and finished up the rest of the joints that were previously inaccessible."}),"\n",(0,t.jsx)(n.p,{children:"Once this was done, we arive at the image below:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/IMG_2819.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"Then we weld the seat back for the chair.\nThis is also fixtured to the table like the sides, but is much less complicated, as there are no fancy angles."}),"\n",(0,t.jsx)(n.p,{children:"At this stage, we need to start grinding down some of the welds.\nAs sad as this is, it is a necessary evil that we must endure.\nThe industrial design gods will be angry at us if we have to many exposed welds.\nThis also improves the fitment of all the tubes that connect both halves of the chair together.\nAfter both sides have been ground down, we can move onto the next stage:"}),"\n",(0,t.jsx)(n.p,{children:"Connecting the sides together!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/IMG_2860.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"The crucial part of this step is ensuring that everything remains square.\nMy strategy for this was making use of a handy 90 degree clamp as well as a bit of elbow grease to flex tubes into place.\nOther than that, this part of the process is fundamentaly the same as the previous steps.\nThe only part that required wierder fixturing was the seat back, as it mounts at a wierd angle in order to conform to the wierd angles that humans form."}),"\n",(0,t.jsx)(n.p,{children:"This leaves us with a finished frame!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/IMG_2870.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"While this isn't a very comfortable chair, or a fun robot yet, it is certainly quite strong!\nIt's already able to withstand the weight of our entire team jumping on it!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/IMG_2868.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"The last step before we have a complete chassis is welding on all the parts that turn this from a chair frame into a robot frame.\nThis includes motor mounts, side panels for the electronics compartment, tabs, tabs, and more tabs!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/4614C96E-88C2-4276-AD32-46FA83D43B99.JPEG",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"With all the tabs welded on, the chair chassis is complete!"})]})}function a(e={}){let{wrapper:n}={...(0,o.useMDXComponents)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(i,{...e})}):i(e)}let h=()=>(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("h2",{children:"Mechanical"}),(0,t.jsx)(a,{})]});function l(e){let n={h4:"h4",img:"img",p:"p",...(0,o.useMDXComponents)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h4,{children:"Design Ideology"}),"\n",(0,t.jsx)(n.p,{children:"The electrical system was designed with repeatability and scalability as its primary goals. This approach allowed for both our chair and our trash can robots to share, fundamentally, the same electronics, despite differences in mechanical structure and motor power requirements"}),"\n",(0,t.jsx)(n.h4,{children:"The Electronics: High Level Overview"}),"\n",(0,t.jsx)(n.p,{children:"From a high level, our electrical systems can be understood through the energy/data flow diagram shown below:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/energy_flow.png",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"Both robots are fully powered off of a single standalone battery, with the trash can having a 12V 7Ah LiFePO4, and the chair having a 12V 5Ah sealed lead-acid battery."}),"\n",(0,t.jsx)(n.p,{children:"Power from the battery is routed through a fuse box, which provides both overcurrent protection and a convenient distribution point for multiple components. From the fuse box, the 12 V rail directly supplies power to the motor controllers, while a DC–DC buck converter steps the voltage down from 12 V to 5 V to power the Raspberry Pi."}),"\n",(0,t.jsx)(n.p,{children:"Then, the Raspberry Pi supplies 3.3V logic power to the magnetic encoders and the logic-side power inputs of the motor controllers via its GPIO headers. The Intel RealSense camera is powered directly from one of the Raspberry Pi’s USB ports, which also carries data."}),"\n",(0,t.jsx)(n.p,{children:"In terms of data flow, the Raspberry Pi communicates wirelessly with an external laptop, which is responsible for high-level robot motion commands. The Raspberry Pi sends its pose data from AprilTags or person-recognition through mediapipe as well as the encoder feedback from the motors. Based on these inputs, it generates motor control signals that are sent to the motor controllers via GPIO pins. Speed is controlled using PWM signals, while motor direction is set using a digital HIGH/LOW control line."}),"\n",(0,t.jsx)(n.h4,{children:"Electronics: Lower-Level Description"}),"\n",(0,t.jsx)(n.p,{children:"Our electronics can be further inspected through our electrical diagram shown below:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"/images/electrical_diagram",alt:""})}),"\n",(0,t.jsx)(n.p,{children:"Power and ground from the battery are connected directly to the main input terminals on the fuse box. Individual fused outputs from the fuse box supply the motor controller and the DC–DC buck converter with power. It should be noted that our motor controller is connected with a 1A fuse on our trash can and a 35A fuse on our chair, due to the high power consumption of the differing motors. The buck converter, however, is protected by a 5A fuse in both systems."}),"\n",(0,t.jsx)(n.p,{children:"Fuse values were selected based on motor datasheet specifications, with additional margin to accommodate the surge current that occurs during motor startup. The 5A fuse for the buck converter was determined by dividing the recommended power supply for the Raspberry Pi, 25W, by our 12V input voltage, to get a value of about 2A. However, the closest fuse value we had on hand was 5A, which we deemed acceptable as we had powered the Raspberry Pi off of a 65W external power supply."}),"\n",(0,t.jsx)(n.p,{children:"The output of the buck converter is then routed through an emergency stop (e-stop) switch to provide an additional layer of safety, and then into the Raspberry Pi’s 5V power input. From the Raspberry Pi, the motor controllers and the magnetic encoders are powered through the 3.3V pins, and the Raspberry Pi’s GPIO connections provide and receive control signals to the motor controllers and from the magnetic encoders. In the wiring diagram, green wires represent digital HIGH/LOW signals used to control motor direction, while blue wires represent PWM signals used for speed control and encoder output."}),"\n",(0,t.jsx)(n.p,{children:"The motors are connected directly to the motor controller outputs, with motor direction being reversed electrically by swapping motor polarity as commanded by the controller. Finally, the camera is connected to the Raspberry Pi via USB, which supplies both power and data connectivity."})]})}function c(e={}){let{wrapper:n}={...(0,o.useMDXComponents)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}let d=()=>(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("h2",{children:"Electrical"}),(0,t.jsx)(c,{})]});function p(e){let n={code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,o.useMDXComponents)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{children:"Software Overview"}),"\n",(0,t.jsx)(n.h2,{children:"Design Goals"}),"\n",(0,t.jsx)(n.p,{children:"Our chief design goals for the project in terms of software were as follows:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"At least two robots (trash can and chair) operating independently and communicating"}),"\n",(0,t.jsx)(n.li,{children:"Computer vision that can recognize an April Tag or Aruco Marker and determine the pose offset of it relative to the camera"}),"\n",(0,t.jsx)(n.li,{children:"Computer vision that can recognize a human to follow"}),"\n",(0,t.jsx)(n.li,{children:"Control code for the motor drivers to make the robots follow a specific target"}),"\n",(0,t.jsx)(n.li,{children:"Software overrides and remote control of the robots from a separate computer"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Additionally, one of our more idealistic goals was to structure the code in a way to be extensible for future use. In essence, we aimed to avoid hard-coding our specific number of robots, IDs of followed markers, and follow target of the robots as much as possible. We also sought to make the environment similar across both robots for the sake of reliability and consistency when testing."}),"\n",(0,t.jsx)(n.h2,{children:"Software Layout"}),"\n",(0,t.jsx)(n.p,{children:"Our final software layout is as follows:"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"images/uml.png",alt:"img"})}),"\n",(0,t.jsxs)(n.p,{children:["All code running on the robots runs within the context of a ROS2 Humble ecosystem, as this was a middleware both of us writing the software were comfortable with. ROS2 is useful for both interprocess and inter-computer communication, and given we had two robots, abstracting communication between them via a middleware familiar to us was a useful choice.\nWe package this code within a ROS2 package, ",(0,t.jsx)(n.code,{children:"chair_robot"}),", contained at ",(0,t.jsx)(n.code,{children:"ros2_ws/src/chair_robot"}),". The package is built using the ",(0,t.jsx)(n.code,{children:"ament_cmake"})," wrapper and can be built by the user by running ",(0,t.jsx)(n.code,{children:"colcon build"})," in the ",(0,t.jsx)(n.code,{children:"ros2_ws"})," directory."]}),"\n",(0,t.jsx)(n.p,{children:"The primary software architecture is described by the code in these files:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"robot_state.py"}),": Defines the overall state machine that the robot runs. Every robot maintains a state throughout its lifetime, determining its behavior. The states are as follows:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"follow": Default following state for the robot. While in this state, it will attempt to drive towards a target by using PID control to reduce its linear and angular error from the setpoint, defined as a position a set distance in front of the follow target, and an angle pointing towards them.'}),"\n",(0,t.jsx)(n.li,{children:'"hold": Autonomous state for when a robot is within a tolerable follow range of the target. The robot will remain still unless its distance from the target leaves a specific range, defined as a threshold offset from the follow distance.'}),"\n",(0,t.jsx)(n.li,{children:'"search": Autonomous state for when a robot has lost its target. The robot holds still while actively looking for a new target. This triggers upon not receiving updates to the target pose for a specific length of time.'}),"\n",(0,t.jsx)(n.li,{children:"\"stop\": Hard stop for the robot, which is not exited autonomously. Triggers upon software override for teleop mode, if the teleop controller passes a 'stop' command or is driving a robot of different ID, as well as if the robot does not receive a heartbeat from the controller for over a second."}),"\n",(0,t.jsx)(n.li,{children:'"teleop": The robot is manually controlled by the controller, and yields its own main loop to instead receive velocity commands from the controller.'}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"pose_from_arcuo.py"}),": Implements code for detecting an Aruco Marker pose using OpenCV's built in functionality. This class has one method, ",(0,t.jsx)(n.code,{children:"process_frame()"}),", which is called by the robot state class that holds a reference to this ",(0,t.jsx)(n.code,{children:"VideoProcess"})," class."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The process frame method reads in a new frame from the camera stream, detects any Aruco markers, identifies their IDs, and broadcasts them via the ",(0,t.jsx)(n.code,{children:"/pose_updates"})," topic."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"pose_from_vision.py"}),": Implements code for detecting a human via MediaPipe. This is implemented as a ROS2 node continually publishing to the ",(0,t.jsx)(n.code,{children:"/pose_updates"})," topic, where the position of the human is estimated by taking the points MediaPipe identifies as defining the hip bone of the person, roughly estimating depth given average human hip length, and determining x/y position via the camera intrinsics and position on the frame. This currently does no recnogition of specific humans nor tracking to ensure the same human is consitently detected."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"transform_helper.py"}),": Implements two classes for dealing with the tf2 transform library, ",(0,t.jsx)(n.code,{children:"StaticTranformBroadcaster"})," and ",(0,t.jsx)(n.code,{children:"FrameUpdater"}),".\n",(0,t.jsx)(n.code,{children:"StaticTranformBroadcaster"})," is a Node initialized at the start of the code execution, which simply broadcasts all static transforms defining the scene. These are transforms relating Aruco Marker locations relative to robots and the target relative to the world frame (which moves with the target). Ideally, this would allow for us to encode offsets of different tags from the wheel center of different robots, or encode the offsets of the cameras as well, though we did not yet implement this.\n",(0,t.jsx)(n.code,{children:"FrameUpdater"})," is a class contained by the state machine, taking a reference to the robot's Node object in order to call ",(0,t.jsx)(n.code,{children:"rclpy"})," methods. The frame updater knows its robot's ID number and transform name, and deals with updating it relative to other transforms upon receiving a transform update.\nWe"]})]})}function u(e={}){let{wrapper:n}={...(0,o.useMDXComponents)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}let m=()=>(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("h2",{children:"Software Section"}),(0,t.jsx)(u,{})]});function f(e){let n={em:"em",h1:"h1",h2:"h2",h3:"h3",p:"p",...(0,o.useMDXComponents)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{children:"Project Management"}),"\n",(0,t.jsx)(n.h2,{children:"Budget"}),"\n",(0,t.jsx)(n.p,{children:"In constructing this project, we were allotted a budget of $250 to spend on obtaining materials.\nIn addition to this provided budget, we made significant use of a number of physical components that we obtained for free, either due to these components coming from our own personal items we had on hand, or due to options to receive or borrow items from others."}),"\n",(0,t.jsx)("iframe",{src:"https://docs.google.com/spreadsheets/d/e/2PACX-1vT_sNQfoW_teaEt_NFqAWRjRmWpaYOzrQvU49fg0_ZSqlX29fgLRRe-pDQ4J4A1h30ysuDitkdMucUW/pubhtml?gid=0&single=true&widget=true&headers=false"}),"\n",(0,t.jsxs)(n.p,{children:['Notes:\n"Found in Robolab" items were unused items salvaged from previous robotics projects, our use of which was cleared with the relevant professor.\n"Personal" items were those that some of us already owned.\nAll "Found in Robolab" and "Personal" items are listed with their ',(0,t.jsx)(n.em,{children:"estimated"})," costs, as we did not have to pay these costs, but the cost is theoretically integrated into our project."]}),"\n",(0,t.jsx)(n.h3,{children:"Budget Reflection"}),"\n",(0,t.jsx)(n.p,{children:"Our team was able to make ample use of materials we already had and could salvage, with many of the more expensive components we used (computers and cameras) either from personal supplies or found. While this appears to make the total cost of our project significant (on the order of 1,000 dollars,) much of this cost was unnecessary were we choosing components from the ground up.\nFor instance, the Raspberry Pi 5 that we used actually impeded some of our progress, as the different hardware didn't support hardware level interrupts via PiGPIO and required us to use a container for running our desired version of ROS2, and the Realsense cameras were overkill for this project --- we didn't use their depth data nor their API/library, we simply grabbed the raw video frames when we wanted."})]})}function w(e={}){let{wrapper:n}={...(0,o.useMDXComponents)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(f,{...e})}):f(e)}let g=()=>(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("h2",{children:"PM Section"}),(0,t.jsx)(w,{})]});function b(){return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)("section",{className:"hero",children:(0,t.jsx)("div",{className:"hero-content",children:(0,t.jsx)("h1",{children:"Desk On Demand"})})}),(0,t.jsx)("section",{id:"overview",className:"card",children:(0,t.jsx)(r,{})}),(0,t.jsx)("section",{id:"mechanical",className:"card",children:(0,t.jsx)(h,{})}),(0,t.jsx)("section",{id:"electrical",className:"card",children:(0,t.jsx)(d,{})}),(0,t.jsx)("section",{id:"software",className:"card",children:(0,t.jsx)(m,{})}),(0,t.jsx)("section",{id:"project-management",className:"card",children:(0,t.jsx)(g,{})})]})}e.s(["default",()=>b],81694)}]);